{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming Connect4Env, CNN_QNetwork, and PERMemory are already defined\n",
        "\n",
        "def compute_reward(board, player):\n",
        "    opponent = 2 if player == 1 else 1\n",
        "    reward = 0\n",
        "    for r in range(6):\n",
        "        for c in range(4):\n",
        "            line = board[r, c:c+4]\n",
        "            if np.sum(line == player) == 3 and np.sum(line == 0) == 1:\n",
        "                reward += 0.5\n",
        "            if np.sum(line == opponent) == 3 and np.sum(line == 0) == 1:\n",
        "                reward += 0.2\n",
        "    return reward\n",
        "\n",
        "def train_dqn(episodes=1000):\n",
        "    env = Connect4Env()\n",
        "    action_size = 7\n",
        "    model = CNN_QNetwork(action_size)\n",
        "    target_model = CNN_QNetwork(action_size)\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    memory = PERMemory(10000)\n",
        "    gamma = 0.99\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_decay = 0.995\n",
        "    batch_size = 64\n",
        "    sync_every = 10\n",
        "\n",
        "    rewards_list = []\n",
        "    epsilons = []\n",
        "    moving_avg_rewards = []\n",
        "    q_values_per_episode = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = random.choice(env.get_valid_actions())\n",
        "            else:\n",
        "                q_values = model(np.expand_dims(state, 0))[0].numpy()\n",
        "                valid_actions = env.get_valid_actions()\n",
        "                q_values_filtered = [q_values[a] if a in valid_actions else -np.inf for a in range(action_size)]\n",
        "                action = np.argmax(q_values_filtered)\n",
        "\n",
        "            next_state, reward, done = env.step(1, action)\n",
        "            shaped_reward = reward + compute_reward(next_state, 1)\n",
        "            total_reward += shaped_reward\n",
        "\n",
        "            q_next = target_model(np.expand_dims(next_state, 0))[0].numpy()\n",
        "            target = shaped_reward + gamma * np.max(q_next) * (1 - int(done))\n",
        "            pred = model(np.expand_dims(state, 0))[0][action].numpy()\n",
        "            error = target - pred\n",
        "\n",
        "            memory.add((state, action, shaped_reward, next_state, done), error)\n",
        "            state = next_state\n",
        "\n",
        "            if len(memory.buffer) >= batch_size:\n",
        "                batch, indices = memory.sample(batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*batch)\n",
        "                states = np.array(states)\n",
        "                next_states = np.array(next_states)\n",
        "\n",
        "                q_next = target_model(next_states).numpy()\n",
        "                q_target = model(states).numpy()\n",
        "\n",
        "                td_errors = []\n",
        "                for i in range(batch_size):\n",
        "                    target_q = rewards[i] + gamma * np.max(q_next[i]) * (1 - int(dones[i]))\n",
        "                    td_errors.append(target_q - q_target[i, actions[i]])\n",
        "                    q_target[i, actions[i]] = target_q\n",
        "\n",
        "                memory.update(indices, td_errors)\n",
        "                model.train_on_batch(states, q_target)\n",
        "\n",
        "        if ep % sync_every == 0:\n",
        "            target_model.set_weights(model.get_weights())\n",
        "\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "        # Track stats ðŸ“Š\n",
        "        rewards_list.append(total_reward)\n",
        "        epsilons.append(epsilon)\n",
        "        if len(rewards_list) >= 10:\n",
        "            moving_avg_rewards.append(np.mean(rewards_list[-10:]))\n",
        "        else:\n",
        "            moving_avg_rewards.append(total_reward)\n",
        "\n",
        "        # Q-values of valid actions at start of episode\n",
        "        with tf.device('/cpu:0'):  # make sure this doesn't throw CUDA memory issues\n",
        "            q_vals = model(np.expand_dims(state, 0))[0].numpy()\n",
        "            avg_q = np.mean([q_vals[a] for a in env.get_valid_actions()])\n",
        "            q_values_per_episode.append(avg_q)\n",
        "\n",
        "        print(f\"Episode {ep}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "    # Save the model after training\n",
        "    model.save('trained_connect4_model.h5')\n",
        "\n",
        "    # ðŸ“ˆ Plotting\n",
        "    def plot_training_curves():\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(rewards_list)\n",
        "        plt.title('Total Reward per Episode')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(epsilons)\n",
        "        plt.title('Epsilon Decay')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Epsilon')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.plot(moving_avg_rewards)\n",
        "        plt.title('Moving Average Reward (window=10)')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Avg Reward')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(2, 2, 4)\n",
        "        plt.hist(rewards_list, bins=30)\n",
        "        plt.title('Reward Distribution')\n",
        "        plt.xlabel('Total Episode Reward')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Additional Q-value tracking\n",
        "        plt.figure()\n",
        "        plt.plot(q_values_per_episode)\n",
        "        plt.title(\"Average Q-Value per Episode\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Q-Value\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    plot_training_curves()\n",
        "\n",
        "\n",
        "def play_with_trained_model():\n",
        "    # Load the trained model\n",
        "    model = tf.keras.models.load_model('trained_connect4_model.h5')\n",
        "\n",
        "    env = Connect4Env()\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "\n",
        "    print(\"Let's play Connect 4! You are Player 1 (X), and the AI is Player 2 (O).\")\n",
        "\n",
        "    while not done:\n",
        "        # Player 1 (User)\n",
        "        action = int(input(\"Enter your move (0-6): \"))\n",
        "        state, reward, done = env.step(1, action)\n",
        "        env.render()\n",
        "\n",
        "        if done:\n",
        "            print(\"Game Over!\")\n",
        "            if reward == 1:\n",
        "                print(\"You win!\")\n",
        "            else:\n",
        "                print(\"It's a tie!\")\n",
        "            break\n",
        "\n",
        "        # AI's turn (Player 2)\n",
        "        print(\"AI's turn...\")\n",
        "        q_values = model(np.expand_dims(state, 0))[0].numpy()\n",
        "        valid_actions = env.get_valid_actions()\n",
        "        q_values_filtered = [q_values[a] if a in valid_actions else -np.inf for a in range(7)]\n",
        "        ai_action = np.argmax(q_values_filtered)\n",
        "\n",
        "        state, reward, done = env.step(2, ai_action)\n",
        "        env.render()\n",
        "\n",
        "        if done:\n",
        "            print(\"Game Over!\")\n",
        "            if reward == 1:\n",
        "                print(\"AI wins!\")\n",
        "            else:\n",
        "                print(\"It's a tie!\")\n",
        "            break\n",
        "\n",
        "# Start training ðŸ§ \n",
        "train_dqn(1000)\n",
        "\n",
        "# After training is complete, let the user play\n",
        "play_with_trained_model()"
      ],
      "metadata": {
        "id": "bHjbc5eQXKqb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}